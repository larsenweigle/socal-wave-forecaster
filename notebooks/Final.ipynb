{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f48cd67-3675-41c7-9d65-b0ae17e22f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f07c9fdb-e035-4cb6-9270-d7a892b09bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH_2015 = '../data/targetstation46240/46240h2015.txt'\n",
    "DATAPATH_2016 = '../data/targetstation46240/46240h2015.txt'\n",
    "DATAPATH_2019 = '../data/targetstation46240/46240h2019.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a76c92-22cc-4a2f-bcf4-5a7fe119e19d",
   "metadata": {},
   "source": [
    "# Format 2019 - 2022 Historical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae61055-9318-4e8e-b974-7f2eb142ab42",
   "metadata": {},
   "source": [
    "For the baseline model, we will only use wave related data as features. We have also downloaded ocean current data from the same buoy location, however, we do not want to over over-featurize given the total size of the dataset and MLP model.\n",
    "\n",
    "Most of the functions in this section format the data text files into dataframes, handle the type conversions, and drop columns we will not use. First, we begin with creating a function `text_to_dataframe` that reads in a datapath and converts it to a pd dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0036c06-2b6d-4c71-b257-f05a5386647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_dataframe(data_path):\n",
    "    \"\"\"\n",
    "    Converts a text file at the given path into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The path to the data file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple where the first element is a pandas Series containing the units \n",
    "               of each column, and the second element is the pandas DataFrame with \n",
    "               the data.\n",
    "    \"\"\"\n",
    "    # Read the data file into a DataFrame\n",
    "    historical_data = pd.read_csv(data_path, delim_whitespace=True, header=0)\n",
    "\n",
    "    # Extract the units row and drop it from the DataFrame\n",
    "    units = historical_data.iloc[0]\n",
    "    historical_data.drop(0, inplace=True)\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    historical_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return units, historical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ccc5d3-906e-41a3-a80e-69d3188be479",
   "metadata": {},
   "source": [
    "There is a little data cleaning we need to do here. As you can see below (where we look at the heads of the dataframes), the values for certain columns are all 99.0 or 9999.0. These values signal a missing value. In the case of our data, the buoy likely does not have the proper instruments to capture these values. To fix this, we will simply drop the columns.\n",
    "\n",
    "Another issue is that we just loaded in a text file, so all the values in the dataframe are strings. The models we train later in the script need numerical data. We can use some useful functions from the pandas library to convert it to a digestable format for our future model.\n",
    "\n",
    "To make window slicing easier for the time series data, I only keep measurements on the start of each hour. The original dataframes contain measurements for every 30 minutes. Given that there are marginal changes (from initial observations) every 30 minutes, we only consider inputs that are seperated by one hour of time for the baseline models. We accomplish this with the last function in the block below.\n",
    "\n",
    "We also drop certain columns to simplify our feature set for the baseline mode. For detailed descriptions of each feature, please refer [here](https://www.ndbc.noaa.gov/faq/measdes.shtml#stdmet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dcf93d5-cd7c-4893-a7c3-7337214db3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(dataframe, columns_to_drop):\n",
    "    \"\"\"\n",
    "    Drop the given columns_to_drop in the dataframe\n",
    "    \"\"\"\n",
    "    dataframe.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "def convert_to_numerical_data(dataframe):\n",
    "    \"\"\"\n",
    "    Convert all columns to numerical values, since our data is passed\n",
    "    in as a .txt\n",
    "    \"\"\"\n",
    "    # Convert all columns to numerical values\n",
    "    dataframe.apply(pd.to_numeric)\n",
    "\n",
    "    # Convert specific columns to integers\n",
    "    columns_to_convert_to_int = ['#YY', 'MM', 'DD', 'hh', 'mm']\n",
    "    for col in columns_to_convert_to_int:\n",
    "        dataframe[col] = dataframe[col].astype(int)\n",
    "\n",
    "def convert_to_hourly(dataframe):\n",
    "    \"\"\"\n",
    "    Convert our data to an hourly format.\n",
    "    \"\"\"\n",
    "    # Drop rows where 'mm' column is not 0\n",
    "    drop_indices = dataframe[dataframe['mm'] != 0].index\n",
    "    dataframe.drop(drop_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658c0c1-30ac-4c00-a3fc-4796dab73e66",
   "metadata": {},
   "source": [
    "For each year, we seem to have some missing values (see the smaller number of rows for the dataframe compared to 2020 and 2022). The missing values likely reflect certain times when the sensors of the buoy were malfuncitoning. Because we are dealing with time series data, where datapoints need to be ordered sequentially, we need to account for these missing rows when processing the dataset.\n",
    "\n",
    "The function below is responsible for finding where the measurement between the ith row and i-1th row is greater than 60 minutes. We will have to account for these rows when preprocessing the data.\n",
    "\n",
    "Note: We check for 60 minute differences after dropping all measurements that are not taken on the hour! See the other functions below to get a better idea of how we accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a15d6741-e5b1-436b-ace4-a41574b7dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_missing_rows(dataframe):\n",
    "    \"\"\"\n",
    "    Identifies missing rows in a DataFrame based on time intervals.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame to check. It must contain columns \n",
    "                                  named '#YY', 'MM', 'DD', 'hh', and 'mm', representing \n",
    "                                  the year, month, day, hour, and minute, respectively.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of indices where the DataFrame has missing rows based on the time \n",
    "              difference between consecutive rows.\n",
    "    \"\"\"\n",
    "    # Rename columns for compatibility with pd.to_datetime()\n",
    "    dataframe = dataframe.rename(columns={\n",
    "        '#YY': 'year',\n",
    "        'MM': 'month',\n",
    "        'DD': 'day',\n",
    "        'hh': 'hour',\n",
    "        'mm': 'minute'\n",
    "    })\n",
    "\n",
    "    # Convert the separate year, month, day, hour, minute columns into a single datetime column\n",
    "    dataframe['datetime'] = pd.to_datetime(dataframe[['year', 'month', 'day', 'hour', 'minute']])\n",
    "\n",
    "    missing_indices = []\n",
    "\n",
    "    # Iterate over rows and check for a time difference of more than 30 minutes\n",
    "    for i in range(1, dataframe.shape[0]):\n",
    "        time_diff = dataframe.iloc[i]['datetime'] - dataframe.iloc[i-1]['datetime']\n",
    "        if time_diff > pd.Timedelta(minutes=60):\n",
    "            missing_indices.append(i)\n",
    "\n",
    "    return missing_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405642d1-5bfd-45c7-bfd2-b447c0fe7c7a",
   "metadata": {},
   "source": [
    "Now, we need to split the data into time series training and label targets. \n",
    "\n",
    "For this step, we need to decide our forecasting window. Given that we are attempting to build an open source wave forecasting model for surfers, we will aim to predict the mean significant wave height in 24-48 hours.\n",
    "\n",
    "Let's start with a 24 hour lead time. With this decided, we can format a dataset for supervised learning. Our x's will be vectors containg the set of features we specify measured at each hour for a certain amount hours. In the code below, we have 4 features measured at each hour for 24 hours. Thus, each x_i will be 96 x 1 ((4 * 24) x 1). The corresponding label will just be a scalar representing the 'WVHT' value 24 hours ahead of the last hour measurements were taken in the x_i vector.\n",
    "We choose our feature set to be wave height (WVHT), dominant wave period (DPD), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fac8f6a3-9086-432b-b439-dbfb296ed920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_supervised_learning_dataset(dataframe, missing_indices, feature_set=['WVHT', 'DPD', 'APD', 'MWD'], window=24, lead=24):\n",
    "    # Convert missing_indices to a set for faster lookup\n",
    "    missing_set = set(missing_indices)\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(dataframe.shape[0] - window - lead + 1):\n",
    "        # Check if any of the indices in the current slice are in the missing set\n",
    "        if any([(i + j) in missing_set for j in range(window)]):\n",
    "            continue\n",
    "        \n",
    "        # Here we grab the window of values for each feature\n",
    "        x_i = dataframe.iloc[i : i + window][feature_set].to_numpy()\n",
    "\n",
    "        # Reshape the array so it can be a row vector of X (1 X 24 * 4) -> (1 x 96)\n",
    "        x_i = x_i.flatten()\n",
    "\n",
    "        # Only fetch the significant wave height value for the label\n",
    "        y_i = dataframe.iloc[i + window + lead - 1]['WVHT']\n",
    "\n",
    "        X.append(x_i)\n",
    "        y.append(y_i)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5e138-ba75-48c0-aae1-e2e5b8a00db9",
   "metadata": {},
   "source": [
    "### 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2669f-dda3-4835-932c-d69c1700b347",
   "metadata": {},
   "source": [
    "We will now explore our data and further clean it before we attempt to predict. We will begin with 2019 and do it cell by cell, but at the end we will generalize and launch it on all our different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae10191c-75cb-4f2f-887c-6c664efc00a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (17299, 18)\n"
     ]
    }
   ],
   "source": [
    "_, historical_data_2019 = text_to_dataframe(DATAPATH_2019)\n",
    "\n",
    "print(f'shape: {historical_data_2019.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f88c4c-cc9a-4bc2-9c2e-0cd3d7184471",
   "metadata": {},
   "source": [
    "We drop WDIR, WSPD, GST, PRES, ATMP, DEWP, VIS, and TIDE because this dataset does not contain those values. TODO: add other dataset to combine with this one in order to fill in those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba30bd24-fb79-4183-a14a-e747ef28e515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#YY</th>\n",
       "      <th>MM</th>\n",
       "      <th>DD</th>\n",
       "      <th>hh</th>\n",
       "      <th>mm</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>WTMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.46</td>\n",
       "      <td>11.11</td>\n",
       "      <td>6.56</td>\n",
       "      <td>346</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>11.76</td>\n",
       "      <td>6.40</td>\n",
       "      <td>351</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>9.88</td>\n",
       "      <td>6.38</td>\n",
       "      <td>343</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.24</td>\n",
       "      <td>9.88</td>\n",
       "      <td>6.63</td>\n",
       "      <td>341</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>10.53</td>\n",
       "      <td>6.20</td>\n",
       "      <td>345</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.11</td>\n",
       "      <td>9.88</td>\n",
       "      <td>6.98</td>\n",
       "      <td>339</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>9.88</td>\n",
       "      <td>7.40</td>\n",
       "      <td>342</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>9.09</td>\n",
       "      <td>7.19</td>\n",
       "      <td>334</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.09</td>\n",
       "      <td>11.11</td>\n",
       "      <td>7.45</td>\n",
       "      <td>343</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>9.88</td>\n",
       "      <td>7.18</td>\n",
       "      <td>338</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.88</td>\n",
       "      <td>7.08</td>\n",
       "      <td>341</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.88</td>\n",
       "      <td>6.39</td>\n",
       "      <td>341</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>11.11</td>\n",
       "      <td>6.56</td>\n",
       "      <td>342</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>11.11</td>\n",
       "      <td>6.81</td>\n",
       "      <td>346</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>9.88</td>\n",
       "      <td>6.25</td>\n",
       "      <td>342</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>10.53</td>\n",
       "      <td>6.79</td>\n",
       "      <td>343</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>11.76</td>\n",
       "      <td>7.20</td>\n",
       "      <td>348</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>11.76</td>\n",
       "      <td>6.81</td>\n",
       "      <td>348</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>11.11</td>\n",
       "      <td>6.96</td>\n",
       "      <td>341</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.53</td>\n",
       "      <td>6.70</td>\n",
       "      <td>342</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>9.88</td>\n",
       "      <td>7.32</td>\n",
       "      <td>339</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>10.53</td>\n",
       "      <td>6.89</td>\n",
       "      <td>338</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>9.88</td>\n",
       "      <td>7.07</td>\n",
       "      <td>338</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.76</td>\n",
       "      <td>8.03</td>\n",
       "      <td>349</td>\n",
       "      <td>13.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     #YY  MM  DD  hh  mm  WVHT    DPD   APD  MWD  WTMP\n",
       "0   2019   1   1   0   0  1.46  11.11  6.56  346  13.6\n",
       "2   2019   1   1   1   0  1.44  11.76  6.40  351  13.5\n",
       "4   2019   1   1   2   0  1.33   9.88  6.38  343  13.5\n",
       "6   2019   1   1   3   0  1.24   9.88  6.63  341  13.5\n",
       "8   2019   1   1   4   0  1.08  10.53  6.20  345  13.5\n",
       "10  2019   1   1   5   0  1.11   9.88  6.98  339  13.4\n",
       "12  2019   1   1   6   0  1.15   9.88  7.40  342  13.4\n",
       "14  2019   1   1   7   0  1.06   9.09  7.19  334  13.4\n",
       "16  2019   1   1   8   0  1.09  11.11  7.45  343  13.4\n",
       "18  2019   1   1   9   0  0.94   9.88  7.18  338  13.4\n",
       "20  2019   1   1  10   0  0.98   9.88  7.08  341  13.3\n",
       "22  2019   1   1  11   0  0.90   9.88  6.39  341  13.3\n",
       "24  2019   1   1  12   0  1.00  11.11  6.56  342  13.3\n",
       "26  2019   1   1  13   0  0.97  11.11  6.81  346  13.2\n",
       "28  2019   1   1  14   0  0.97   9.88  6.25  342  13.2\n",
       "30  2019   1   1  15   0  0.87  10.53  6.79  343  13.3\n",
       "32  2019   1   1  16   0  0.87  11.76  7.20  348  13.2\n",
       "34  2019   1   1  17   0  0.77  11.76  6.81  348  13.2\n",
       "36  2019   1   1  18   0  0.83  11.11  6.96  341  13.2\n",
       "38  2019   1   1  19   0  0.80  10.53  6.70  342  13.2\n",
       "40  2019   1   1  20   0  0.84   9.88  7.32  339  13.3\n",
       "42  2019   1   1  21   0  0.83  10.53  6.89  338  13.3\n",
       "44  2019   1   1  22   0  0.76   9.88  7.07  338  13.3\n",
       "46  2019   1   1  23   0  0.75  11.76  8.03  349  13.3"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cols(historical_data_2019, ['WDIR', 'WSPD', 'GST', 'PRES', 'ATMP', 'DEWP', 'VIS', 'TIDE'])\n",
    "\n",
    "convert_to_numerical_data(historical_data_2019)\n",
    "\n",
    "# Later, could convert to every 4hr or 12hr\n",
    "convert_to_hourly(historical_data_2019)\n",
    "\n",
    "# See first 24 hours of dataset\n",
    "historical_data_2019.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "440c98e3-2a72-4925-8db8-82c6b76c13e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_indices_2019 = check_for_missing_rows(historical_data_2019)\n",
    "\n",
    "len(missing_indices_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e7c6e7c-d1b0-4fa0-b142-0e8826117e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2019, y_2019 = build_supervised_learning_dataset(historical_data_2019, missing_indices_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3658b-0985-48c6-972b-0241aed67de8",
   "metadata": {},
   "source": [
    "### 2015 and other years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fcb71e9f-cf7e-49db-83d3-de6eba7aec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(path_to_data, buoy_specs):\n",
    "    _, historical_data = text_to_dataframe(path_to_data)\n",
    "\n",
    "    drop_cols(historical_data, buoy_specs)\n",
    "\n",
    "    convert_to_numerical_data(historical_data)\n",
    "    \n",
    "    # Later, could convert to every 4hr or 12hr\n",
    "    convert_to_hourly(historical_data)\n",
    "    \n",
    "    # See first 24 hours of dataset\n",
    "    print(historical_data.head(24))\n",
    "\n",
    "    missing_indices = check_for_missing_rows(historical_data)\n",
    "    len(missing_indices)\n",
    "\n",
    "    X, y = build_supervised_learning_dataset(historical_data, missing_indices)\n",
    "    return X_2019, y_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76f944ee-05fb-4d71-9f66-0602a5f6d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [#YY, MM, DD, hh, mm, WVHT, DPD, APD, MWD, WTMP]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "X_2015, y_2015 = preprocess_data(DATAPATH_2015, ['WDIR', 'WSPD', 'GST', 'PRES', 'ATMP', 'DEWP', 'VIS', 'TIDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8dd69-22e7-4aad-b8ca-28125f881d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
